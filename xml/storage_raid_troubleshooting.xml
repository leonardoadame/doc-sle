<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-raidtroubleshooting" xml:lang="en">
 <title>Troubleshooting software RAIDs</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
  <para>
   Check the <filename>/proc/mdstat</filename> file to find out whether a RAID
   partition has been damaged. If a disk fails, 
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <command>mdadm /dev/mdX --add
   /dev/sdX</command>. Replace <literal>X</literal> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID&nbsp;0).
  </para>

  <para>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </para>

  <sect1 xml:id="sec-raid-trouble-autorecovery">
   <title>Recovery after failing disk is back again</title>
   <para>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Problems with the disk media.
     </para>
    </listitem>
    <listitem>
     <para>
      Disk drive controller failure.
     </para>
    </listitem>
    <listitem>
     <para>
      Broken connection to the disk.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In the case of disk media or controller failure, the device needs to be
    replaced or repaired. If a hot spare was not configured within the RAID,
    then manual intervention is required.
   </para>
   <para>
    In the last case, the failed device can be automatically re-added with the
    <command>mdadm</command> command after the connection is repaired (which
    might be automatic).
   </para>
   <para>
    Because <command>md</command>/<command>mdadm</command> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </para>
   <para>
    Under some circumstances&mdash;such as storage devices with an internal
    RAID array&mdash;connection problems are very often the cause of the
    device failure. In such case, you can tell <command>mdadm</command> that it
    is safe to automatically <option>--re-add</option> the device after it
    appears. You can do this by adding the following line to
    <filename>/etc/mdadm.conf</filename>:
   </para>
<screen>POLICY action=re-add</screen>
   <para>
    Note that the device will be automatically re-added after re-appearing only
    if the <systemitem>udev</systemitem> rules cause <command>mdadm -I
    <replaceable>DISK_DEVICE_NAME</replaceable></command> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </para>
   <para>
    If you want this policy to only apply to some devices and not to the
    others, then the <literal>path=</literal> option can be added to the
    <literal>POLICY</literal> line in <filename>/etc/mdadm.conf</filename> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <command>man 5 mdadm.conf</command>
    for more information.
   </para>

   <sect2 xml:id="sec-replacing-failed-drive">
    <title>Replacing a failed drive</title>
    <note>
      <title>RAID 1 not covered</title>
      <para>
        The following instructions does not cover replacing a disk from RAID 1.
      </para>
    </note>
    <important>
      <title>The replacement disk without data and partitions</title>
      <para>
        Make sure that the replacement disk does not contain any data or partitions. The data or paritions on the disk will be erased.
      </para>
    </important>
    <para>
      Before making any changes to the array, perform several checks as described below.
    </para>
    <procedure>
      <step>
    <para>
      View the current state of the array by running:
    </para>
    <screen>&prompt.root;<command>cat /proc/mdstat</command></screen>
    <para>
      In the output a missing disk is indicated by <literal>[_U]</literal> or <literal>[U_]</literal>. Usually, the broken disks are not visible in the command output.
    </para>

    <note>
      <title>The md number may differ in your case</title>
      <para>
        Note down which partition number is assigned to which md number.
      </para>
    </note>
  </step>
  <step>
    <para>
      Using the <command>lsblk</command> command, check the hierarchical structure of each disk and partition:
    </para>
    <screen>&prompt.root;<command>lsblk</command>

NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS
sr0      11:0    1 1024M  0 rom   
vda     254:0    0   60G  0 disk  
├─vda1  254:1    0    8M  0 part  
├─vda2  254:2    0   20G  0 part  /boot/grub2/x86_64-efi
│                                 /srv
│                                 /home
│                                 /opt
│                                 /boot/grub2/i386-pc
│                                 /usr/local
│                                 /boot/writable
│                                 /.snapshots
│                                 /root
│                                 /
└─vda3  254:3    0   40G  0 part  /var
vdb     254:16   0   20G  0 disk  /run/media/root/ext4
vdc     254:32   0   10G  0 disk  
└─md127   9:127  0   20G  0 raid0 /run/media/root/raid0
vdd     254:48   0   10G  0 disk  
    </screen>
    <para>
      The command above shows the hierarchy related to the md devices and show the current use. 
    </para>
  </step>
  <step>
    <para>
      Make sure, that the disk to be replaced (<literal>vdd</literal>) in the example is not showing any subelements. Run the command:
    </para>
    <screen>&prompt.root;<command>blkid</command>

...
/dev/vdc: UUID="27a652e1-2567-1802-ccf3-6dab3b40bfd7" UUID_SUB="322fb559-a4db-cedc-5109-b9ba0fd8312a" LABEL="localhost.localdomain:raid0" TYPE="linux_raid_member"
...
    </screen>
    <para>
      The command displays the current UUIDs of all block devices.
</para>
<step>
<para>
       Make sure that the disk you want to use as a replacement does not contain any partitions:
    </para>
    <screen>&prompt.root;<command>parted /dev/vdd print</command>
      
Model: Virtio Block Device (virtblk)                                      
Disk /dev/vdd: 10.7GB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags: 
    </screen>
  </step>
  <step>
    <para>
      Check which block device is used for booting:
    </para>
    <screen>&prompt.root;efibootmgr -v</screen>
    <para>
      Save the output of the commands above for possible later use.
    </para>
  </step>
  </procedure>
  <para>
    The following procedure describes replacing of the disk.
  </para>
  <procedure>
    <step>
      <para>
        If the broken disk is still present in the array, mark each partition of the disk as failed:
      </para>
      <screen>&prompt.root;<command>mdadm --manage /dev/md127 --fail /dev/vdd<replaceable>PARTITION_ID</replaceable></command>

      mdadm: set /dev/vdd1 faulty in /dev/md127
      </screen>
    </step>
    <step>
      <para>
        Remove all partitions of the broken disk from the array. For each partition, run:
      </para>
      <screen>&prompt.root;<command>mdadm --manage /dev/md127 --remove /dev/vdd<replaceable>PARTITION_ID</replaceable></command>

      mdadm: hot removed /dev/vdd1 from /dev/md127
      </screen>
    </step>
  </procedure>
   </sect2>
  
 </sect1>
 
 </chapter>
